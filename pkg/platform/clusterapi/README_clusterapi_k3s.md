# Cluster API Testing with K3S

This readme covers setting up a cluster API management cluster
with the metal3 infrastructure provider, using libvirt VMs as
bare metal servers. The cluster API management cluster runs in
k3s here, but any Kubernetes cluster can be used - just be
careful of networking issues when using kubernetes in docker
like Kind or k3d.

## K3S Install

### (Optional) If cluster needs to be reached behind a NAT

```
myip=$(curl 'https://api.ipify.org')
echo "Using external IP $myip"
```

### Install K3S

This disables traefik, and registers the external IP as a valid
IP SANS on the kubeconfig certificate.

```
export K3S_KUBECONFIG_MODE="644"
export INSTALL_K3S_EXEC="server --disable=traefik --tls-san $myip"

curl -sfL https://get.k3s.io | sh -
```

### (Optional) Generate a kubeconfig based on the external IP

By default the kubeconfig generated by the k3s install uses a
local IP. To make this reachable via the external IP, we make a copy
and updated the server IP.

```
cp /etc/rancher/k3s/k3s.yaml .
kubectl config set-cluster default --server=https://${myip}:6443 --kubeconfig=k3s.yaml
kubectl config set-context k3s --cluster=default --namespace=default --user=default --kubeconfig=k3s.yaml
kubectl config use-context k3s --kubeconfig=k3s.yaml
kubectl config delete-context default --kubeconfig=k3s.yaml
```

### Install cert-manager

The cluster API controllers need cert-manager installed.

```
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.19.0/cert-manager.yaml
```

## Set up Virtual Machines

These virtual machines act as bare metal servers. We'll use libvirt
to manage them. These instructions assume Ubuntu 24.04.

### Install libvirt
```
sudo apt update
sudo apt install qemu-kvm libvirt-daemon-system virt-manager bridge-utils
```

### Create libvirt network

Libvirt networks are NAT'd by default, which allows outgoing traffic
but not incoming traffic. You can also create a routed network, but
you'll have take care of creating routes on your other machines or
routers yourself.

Nat'd network:
```
cat > net.xml << EOF
<network>
  <name>baremetal</name>
  <forward mode='nat'>
    <nat>
      <port start='1024' end='65535'/>
    </nat>
  </forward>
  <bridge name='metal3'/>
  <ip address='192.168.222.1' netmask='255.255.255.0'>
    <dhcp>
      <range start="192.168.222.10" end="192.168.222.100"/>
      <host mac="00:60:2f:31:81:01" ip="192.168.222.101" name="node1"/>
      <host mac="00:60:2f:31:81:02" ip="192.168.222.102" name="node2"/>
    </dhcp>
  </ip>
</network>
EOF
```

Routed network:
```
cat > net.xml << EOF
<network>
  <name>baremetal</name>
  <forward mode='route' />
  <bridge name='metal3'/>
  <ip address='192.168.222.1' netmask='255.255.255.0'>
    <dhcp>
      <range start="192.168.222.10" end="192.168.222.100"/>
      <host mac="00:60:2f:31:81:01" ip="192.168.222.101" name="node1"/>
      <host mac="00:60:2f:31:81:02" ip="192.168.222.102" name="node2"/>
    </dhcp>
  </ip>
</network>
EOF
```

Start the network:
```
sudo virsh -c qemu:///system net-define net.xml
sudo virsh -c qemu:///system net-start baremetal
```

If desired, set to auto start on reboot
```
sudo virsh net-autostart baremetal
```

Other helpful commands:
```
# list networks
sudo virsh net-list --all
# get network info
sudo virsh net-info baremetal
sudo virsh net-dumpxml baremetal
# destroy network (when done)
sudo virsh -c qemu:///system net-destroy baremetal
sudo virsh -c qemu:///system net-undefine baremetal
```

If routed, add routes. Assumes VMs are on machine 192.168.50.143:
```
# linux
ip route add 192.168.222.0/24 via 192.168.50.143 dev eth0
# windows
route add 192.168.222.0 mask 255.255.255.0 192.168.50.143
```

### Create VMs

This creates two VMs to simulate bare metal hosts.
Note that the Ubuntu image we're using here requires 4096MB of RAM
to boot from ramdisk.

```
sudo virt-install \
  --connect qemu:///system \
  --name node1 \
  --description "Virtualized BareMetalHost" \
  --osinfo=ubuntu-lts-latest \
  --ram=4096 \
  --vcpus=2 \
  --disk size=25 \
  --graphics=none \
  --console pty \
  --serial pty \
  --pxe \
  --network network=baremetal,mac="00:60:2f:31:81:01" \
  --noautoconsole
sudo virt-install \
  --connect qemu:///system \
  --name node2 \
  --description "Virtualized BareMetalHost" \
  --osinfo=ubuntu-lts-latest \
  --ram=4096 \
  --vcpus=2 \
  --disk size=25 \
  --graphics=none \
  --console pty \
  --serial pty \
  --pxe \
  --network network=baremetal,mac="00:60:2f:31:81:02" \
  --noautoconsole
```

Other helpful commands:
```
# check that they are present in virsh
sudo virsh list --all

# to delete the VM
sudo virsh destroy node1
sudo virsh undefine node1 --nvram
sudo virsh destroy node2
sudo virsh undefine node2 --nvram
```

## Image Server

The image server will serve the OS install image over https.
Nothing too fancy here. We run this on the machine itself
but it could also run inside k3s as a container.

```
# set up image server
# https://book.metal3.io/quick-start#image-server
# https://artifactory.nordix.org/ui/native/metal3/images/k8s_v1.34.1/
mkdir disk-images
cd disk-images
wget https://artifactory.nordix.org/artifactory/metal3/images/k8s_v1.34.1/UBUNTU_24.04_NODE_IMAGE_K8S_v1.34.1.qcow2
# get checksum for cluster api
sha256sum UBUNTU_24.04_NODE_IMAGE_K8S_v1.34.1.qcow2
cd ..
docker run --name image-server --rm -d -p 80:8080 \
  -v "$(pwd)/disk-images:/usr/share/nginx/html" nginxinc/nginx-unprivileged
# IP to reach the image server from the VM
export IMGSRV_IP=192.168.50.143
```

## Sushy BMC Emulator

This sets up sushy-tools to emulate the BMC for the VMs.
In this example we're going to use redfish instead of PIXE to
boot the VMs over IPMI.

Sushy-tools use python3. Install packages:
```
sudo apt install -y libvirt-dev pkg-config python3-dev
```

Set up environment:
```
mkdir sushytools; cd sushytools
python3 -m venv .venv
# (run activate every time you need to run sushy-tools)
source .venv/bin/activate
pip install sushy-tools
pip install libvirt-python
# set up self-signed cert for https (once)
openssl req -x509 -sha256 -newkey rsa:2048 -keyout key.pem -days 1000 -nodes -out cert.pem
```

Run emulator:
```
sushy-emulator --libvirt-uri qemu:///system --port 8000 -i 0.0.0.0 --ssl-key key.pem --ssl-certificate cert.pem
```

Test:
```
curl -k https://localhost:8000/redfish/v1/Systems/
```

## Set up Ironic

Ironic interacts with the BMC. Metal3 leverages it for all BMC related
control. See:
- https://book.metal3.io/irso/introduction
- https://book.metal3.io/irso/install-basics

Install via manifest:
```
IRSO_VERSION=0.6.0
kubectl apply -f \
    https://github.com/metal3-io/ironic-standalone-operator/releases/download/v$IRSO_VERSION/install.yaml
kubectl wait --for=condition=Available --timeout=120s \
  -n ironic-standalone-operator-system deployment/ironic-standalone-operator-controller-manager
```

Set up CR config.
Note that the IP is how the ironic service will be reach from the
bare metal machine's BMC.
```
IRONIC_IP=192.168.50.143
IRONIC_API_PORT=6385
IRONIC_IMG_PORT=6180
PROV_IF=virbr0

cat > ironic-virtualmedia.yaml << EOF
apiVersion: ironic.metal3.io/v1alpha1
kind: Ironic
metadata:
  name: ironic
  namespace: ironic-standalone
spec:
  version: "30.0"
  networking:
    interface: $PROV_IF
    externalIP: $IRONIC_IP
    apiPort: $IRONIC_API_PORT
    imageServerPort: $IRONIC_IMG_PORT
  deployRamdisk:
    extraKernelParams: console=ttyS0
EOF
```

Apply config:
```
kubectl create ns ironic-standalone
kubectl apply -f ironic-virtualmedia.yaml
```

Other helpful commands:
```
# when k3s machine reboots, ironic doesn't always come up properly
kubectl rollout restart deploy -n ironic-standalone ironic-service
```

Test:
```
curl http://$IRONIC_IP:$IRONIC_API_PORT/
```

### Get Ironic username and password

```
export IRONIC_NS=ironic-standalone
export IRONIC_SECRET=$(kubectl -n $IRONIC_NS get ironic -o json | jq -r ".items[0].spec.apiCredentialsName")
export IRONIC_USER=$(kubectl -n $IRONIC_NS get secret $IRONIC_SECRET -o json | jq -r ".data.username" | base64 --decode)
export IRONIC_PASS=$(kubectl -n $IRONIC_NS get secret $IRONIC_SECRET -o json | jq -r ".data.password" | base64 --decode)
```

## Metal3 Operator

The metal3 operator allows for provisioning bare metal hosts
using metal3-specific CRDs.
See https://book.metal3.io/bmo/introduction.html.

Install the operator:
```
git clone https://github.com/metal3-io/baremetal-operator.git
cd baremetal-operator

# create namespace so we can create ironic credentials secret
kubectl create ns baremetal-operator-system
kubectl create secret generic -n baremetal-operator-system ironic-credentials \
  --from-literal=username=$IRONIC_USER --from-literal=password=$IRONIC_PASS
# if ironic is on a different cluster, on that cluster use
kubectl create secret generic -n baremetal-operator-system ironic-credentials \
  --from-literal=username=$IRONIC_USER --from-literal=password=$IRONIC_PASS \
  --dry-run=client --output=yaml > ironic-creds.yaml
```

Generate the manifest:
```
# set kustomize options to point to ironic configs
# IRONIC_ENDPOINT points to the ironic-api service
# CACHEURL points to the image server
# For the real deal, LOCALIP needs to point the hostname to access
# the cluster ingress to reach the ironic-api service.
export CFG_DIR=config/overlays/edgexr
mkdir -p $CFG_DIR

cat > $CFG_DIR/ironic.env << EOF
DEPLOY_KERNEL_URL=http://$IRONIC_IP:$IRONIC_IMG_PORT/images/ironic-python-agent.kernel
DEPLOY_RAMDISK_URL=http://$IRONIC_IP:$IRONIC_IMG_PORT/images/ironic-python-agent.initramfs
PROVISIONING_INTERFACE=
DHCP_RANGE=172.22.0.10,172.22.0.100
IRONIC_ENDPOINT=http://$IRONIC_IP:$IRONIC_API_PORT/v1/
CACHEURL=http://$IMGSRV_IP
EOF

cat > $CFG_DIR/kustomization.yaml << EOF
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: baremetal-operator-system

resources:
  - ../../namespace
  - ../../base

components:
  - ../../components/basic-auth

generatorOptions:
  disableNameSuffixHash: true

configMapGenerator:
- name: ironic
  behavior: create
  envs:
  - ironic.env
EOF

kustomize build $CFG_DIR > bmo.yaml
```

Apply the manifest:
```
kubectl apply -f bmo.yaml
```

## Cluster API operator with metal3

This will install the cluster API operator, along with the
metal3-specific infrastructure provider.
See https://cluster-api.sigs.k8s.io/

Install clusterctl:
```
curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.11.2/clusterctl-linux-amd64 -o clusterctl
sudo install -o root -g root -m 0755 clusterctl /usr/local/bin/clusterctl
```

Install the cluster API operator and metal3 cluster API provider.
This defaults to kubeadm for bootstrap and control-plane providers.
```
clusterctl init --infrastructure metal3:v1.11.0
```

(Optional) TODO: support rke2 as well
```
clusterctl init --bootstrap rke2:v0.20.1 --control-plane rke2:v0.20.1
```

To delete capm3:
```
clusterctl delete --infrastructure metal3
```

The metal ip-address-manager is a requirement:
```
git clone https://github.com/metal3-io/ip-address-manager.git
cd ip-address-manager
make deploy
# install metal3 ipam crds
# kubectl apply -f https://github.com/metal3-io/ip-address-manager/releases/download/v1.11.0/ipam-components.yaml
```

## Set up Bare Metal Hosts

To tell metal3 about the bare metal hosts, the operator needs to create
a bare metal host object for each bare metal host.

```
# whereever the BMC is running (sushy-tools)
BMC_IP=192.168.50.143
```

Set up env vars for generating node yaml:
```
# node1
export NODE=node1
export UUID=$(curl -s -L -k https://$BMC_IP:8000/redfish/v1/Systems/node1 | jq -r .UUID)
export MACADDR=00:60:2f:31:81:01
export NAMESPACE=dc1
# node2
export NODE=node2
export UUID=$(curl -s -L -k https://$BMC_IP:8000/redfish/v1/Systems/node2 | jq -r .UUID)
export MACADDR=00:60:2f:31:81:02
export NAMESPACE=dc1
```

Generate node yaml (run once per node)
```
cat > $NODE.yaml << EOF
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: $NODE
  namespace: $NAMESPACE
spec:
  online: true
  bootMACAddress: $MACADDR
  bootMode: legacy
  bmc:
    address: "redfish-virtualmedia+https://$BMC_IP:8000/redfish/v1/Systems/$UUID"
    credentialsName: $NODE-bmc-secret
    disableCertificateVerification: true
  hardwareProfile: libvirt
---
apiVersion: v1
kind: Secret
metadata:
  name: $NODE-bmc-secret
  namespace: $NAMESPACE
type: Opaque
data:
  username: YWRtaW4=       # base64("admin")
  password: cGFzc3dvcmQ=   # base64("password")
EOF
```

and apply:
```
kubectl apply -f $NODE.yaml
```
